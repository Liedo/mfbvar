---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```
# News (2017-03-11)
What is new in version 0.2.1:

- Methods (`print`, `summary`, `plot`, `predict`)
- Better organization of names, argument order. This may make it incompatible with older code

# Example file

This short example illustrates estimation of the model.

## Data generation
First, we generate some dummy data as a VAR(1) with three variables whose uncondtional means are all zero.
```{r}
library(mfbvar)
TT <- 200
n_vars <- 3
set.seed(100)

Y <- matrix(0, 2*TT, n_vars)
Phi <- matrix(c(0.3, 0.1, 0.2, 0.3, 0.3, 0.6, 0.2, 0.2, 0.3), 3, 3)
for (i in 2:(2*TT)) {
  Y[i, ] <- Phi %*% Y[i-1,] + rnorm(n_vars)
}
Y[, n_vars] <- zoo::rollapply(Y[, n_vars], 3, mean, fill = NA, align = "right")
Y <- Y[-(1:TT),]
Y[setdiff(1:TT, seq(1, TT, 3)), n_vars] <- NA

dates <- paste(rep(2000:2017, each = 12), "-", 1:12, sep = "")
Y <- as.data.frame(Y)
rownames(Y) <- dates[1:nrow(Y)]
colnames(Y) <- c("GDP", "Infl", "Interest")
```

The data now looks like this:
```{r}
head(Y)
```
The names are, of course, made up, but this is to illustrate how the names are used later on.

## Settings and priors
We next need to make some settings for the estimation:
```{r}
n_burnin <- 2000
n_reps <- 2000
n_fcst <- 8
n_lags <- 4
n_vars <- ncol(Y)
n_T <- nrow(Y)
```

The `n_*` variables are self-explanatory. Next, create the matrix of deterministic terms (also for the forecasting period):
```{r}
d <- matrix(1, nrow = n_T, ncol = 1, dimnames = list(1:nrow(Y), "const"))
d_fcst <- matrix(1, nrow = n_fcst, ncol = 1, 
                 dimnames = list(dates[(nrow(Y)+1):(nrow(Y)+n_fcst)], "const"))
d_fcst
```

For the prior on the dynamic coefficients and the error covariance matrix, we need to set the prior degrees of freedom as well as the prior mean of AR(1) coefficients and the tuning parameters:
```{r}
prior_nu <- n_vars + 2 
prior_Pi_AR1 <- c(0, 0, 0) 
lambda1 <- 0.1
lambda2 <- 1
```
The prior on the steady states also needs to be set:
```{r}
prior_psi_mean <- c(0, 0, 0) 
prior_psi_Omega <- c(0.5, 0.5, 0.5) 
prior_psi_Omega <- diag((prior_psi_Omega / (qnorm(0.975, mean = 0, sd = 1)*2))^2) 
```
The third line simply converts the length of the prior interval to the variance in a normal distribution.

Finally, we also need to create the matrix that relates unobservables to observables. In this example, the first two variables are assumed to be observed every period, whereas the third is assumed to be observed every third time period. Moreover, when it is observed, we observe the average over three periods. This can be specified using the `build_Lambda()` function:
```{r}
Lambda <- build_Lambda(c("identity", "identity", "average"), n_lags)
```

## Main call
After having set these preliminary variables, we can now call the main function `mfbvar()`:
```{r, cache = TRUE, results = "hide"}
set.seed(10237)
mfbvar_obj <- mfbvar(Y, d, d_fcst, Lambda, prior_Pi_AR1, lambda1, lambda2, 
                     prior_nu, prior_psi_mean, prior_psi_Omega, 
                     n_lags, n_fcst, n_burnin, n_reps) 
```


## Obtaining the results
Four S3 methods are implemented:

```{r methods, fig.width = 10, fig.asp = 0.5}
mfbvar_obj
summary(mfbvar_obj)
predict(mfbvar_obj, tidy = TRUE)
plot(mfbvar_obj) 
```

## Marginal data density
The package contains functions for estimating the marginal data density. This is most useful when done in parallel, so first we can set up a cluster and then compute the marginal data density for various values of the hyperparameters `lambda1` and `lambda2`.

First, we'll use grids between 0.1 and 0.5 for `lambda1` and between 1 and 4 for `lambda2`.
```{r, cache = TRUE}
lambda1_vec <- seq(0.1, 0.5, by = 0.05)
lambda2_vec <- seq(1, 4, by = 0.5)
lambda_mat <- expand.grid(lambda1_vec, lambda2_vec)
```
We can also create two wrapper functions to use for the parallel call:
```{r, cache = TRUE}
mdd_search <- function(lambda1, lambda2) {
  mfbvar_obj <- mfbvar(Y, d, d_fcst, Lambda, prior_Pi_AR1, lambda1, lambda2, 
                     prior_nu, prior_psi_mean, prior_psi_Omega, 
                     n_lags, n_fcst, n_burnin, n_reps) 
  log_mdd <- mdd2(mfbvar_obj, p_trunc = 0.5)$log_mdd
  return(list(log_mdd = log_mdd, lambda1 = lambda1, lambda2 = lambda2))
}

par_func <- function(j, lambda_mat) {
  lambda1 <- lambda_mat[j, 1]
  lambda2 <- lambda_mat[j, 2]
  mdd_search(lambda1, lambda2)
}
```
Finally, the parallel processing is conducted using `parLapply()`:

```{r mdd, cache = TRUE, include = FALSE}
library(parallel)
library(doParallel)
cl <- makePSOCKcluster(7)
clusterSetRNGStream(cl, iseed = 2983468)
registerDoParallel(cl)
clusterExport(cl = cl, varlist = setdiff(ls(), c("cl", "lambda1", "lambda2")), envir = environment())
# Load the package
clusterEvalQ(cl, {
  library(mfbvar)
})

mdd_res <- parLapply(cl = cl, X = 1:nrow(lambda_mat), fun = par_func, lambda_mat = lambda_mat)
stopCluster(cl)
```

Transforming the results and plotting gives us an idea of what hyperparameter values are sensible.
```{r mddplot, fig.width = 5, fig.asp = 0.8}
library(ggplot2)
mdd <- data.frame(matrix(unlist(mdd_res), ncol = 3, byrow = TRUE))
names(mdd) <- c("log_mdd", "lambda1", "lambda2")
ggplot(mdd, aes(x = lambda1, y = lambda2, fill = log_mdd)) +
  geom_tile() +
  theme_minimal() +
  scale_fill_gradient(low = "grey20", high = "grey90", name = "log(mdd)") +
  coord_fixed(ratio = 1/10)
```

## Profiling
Profiling of the code shows that `simulation_smoother` is by far the most time-consuming part of the code. 
```{r profvis, cache = TRUE, include = FALSE}
library(profvis)
profiling <- profvis({mfbvar_obj <- mfbvar(Y, d, d_fcst, Lambda, prior_Pi_AR1, lambda1, lambda2, 
                     prior_nu, prior_psi_mean, prior_psi_Omega, 
                     n_lags, n_fcst, n_burnin, n_reps) }, prof_output = "../profiling.Rprof")
```

```{r profiling}
library(tidyverse)
profiling <- summaryRprof("../profiling.Rprof")$by.total
profiling <- profiling[order(profiling$total.pct, decreasing = TRUE) & profiling$total.pct < 99,]
profiling <- head(profiling) 
profiling$call <- rownames(profiling)
profiling <- as_tibble(profiling)
profiling %>%
  ggplot(aes(x = reorder(call, total.pct), y = total.pct)) +
  geom_bar(stat = "identity", width = 0.25) +
  theme_minimal() +
  coord_flip() +
  labs(y = "Percent", x = "Function call", title = "Most expensive functions calls in mfbvar")
```

## To do
Some things that remain to do:

- Wrapper for computing the mdd for grids of values, possibly also in parallel. Something like `mdd_search(lambda1, lambda2, n_cores = 1, method, ...)`.
- Helper function `interval_to_moments()` which takes a matrix of lower and upper bounds for prior intervals for steady states and return `prior_psi_mean` and `prior_psi_Omega`.
